section{Resultados y Discusión}

Gracias a la experimentación realizada en \ref{hipótesis1}, \ref{hipótesis2}, \ref{hipótesis3}, \ref{hipótesis4} y \ref{seccion 4.0.3} es posible concluir una serie de parámetros, los cuales permiten optimizar las operaciones en cuestión de exactitud y de tiempos, notar que para obtener la mejor exactitud se debe tomar la decisión de fijar las frecuencia mínima o las máximas con la que aparecen las palabras, es decir un parámetro deberá ser el óptimo y el otro estará fijado como en la experimentación, ya que si se utilizan ambos óptimos se corre el riesgo de estar podando demasiado las reseñas. También de la experimentación en la sección \ref{seccion 4.0.3} pudimos establecer un épsilon capaz de brindarnos un criterio de parada para el \textbf{método de la potencia} ya que la implementación tuvo grandes problemas en cuestión de  tiempos en los que el algoritmo corrió. Se pudo concluir y afirmar basado en la experimentación una reducción importante en los tiempos de computo de \textbf{Knn} gracias a la utilización del \textbf{PCA.}  

Cómo se planteó antes, el incremento de $m$ brinda una mayor cantidad de palabras (esto mediante el CountVectorizer) para describir las reseñas tratando de generar un análisis más preciso de las reseñas generando la posibilidad de establecer vínculos más exactos entre reseñas, al tener más dimensiones para comparar.

Un pro de este método es que es simple aprender e implementar y como contra es que utiliza todo el conjunto de datos para entrenar y esto en cierta medida consume muchos recursos de la computadora, como son la memoria y la CPU, es por esto que se recomienda utilizarlo con datasets medianamente pequeños o sin una gran cantidad de features(ó columnas).

Como trabajo futuro a implementar pensamos probar nuestras hipótesis sobre otros conjuntos de datos, como por ejemplo el reconocimiento de dígitos visto en el laboratorio, cómo por ejemplo variar el $k$ y verificar qué datos obtenemos.

